{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "ht_occurences = Counter([])\n",
    "ht_texts = defaultdict(list)\n",
    "lang_occurences = Counter([])\n",
    "\n",
    "j=0\n",
    "with open(r\"C:\\Users\\Shmuli\\Downloads\\twt_relevant_13_05_20.json\", encoding='utf-8') as f:\n",
    "    try:\n",
    "        for i, line in enumerate(f):\n",
    "           # print('line:' ,i ,\" : \", line)\n",
    "            line = line.replace(\",\\n\",\"\")\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                full_text = data['doc']['full_text'].lower()\n",
    "                if 'coronavirus' in full_text or 'covid' in full_text or True:\n",
    "                    docs.append(data['doc'])\n",
    "                    if 'relevance' in data['doc']:\n",
    "                        lang_occurences[data['doc']['lang']] += 1\n",
    "                        ht = re.findall(r\"#(\\w+)\",data['doc']['full_text'].lower())\n",
    "                        for hash_tag in ht:\n",
    "                            ht_occurences[hash_tag] += 1\n",
    "                            ht_texts[hash_tag].append(j)\n",
    "                    j+=1     \n",
    "            except ValueError as e:\n",
    "                print(f\"Malformed JSON at {i}\")\n",
    "              \n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_occurences.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_occurences.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(df.coordinates.isnull().sum()-df.shape[0])/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keywords.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_type_all = Counter()\n",
    "point_place_mask = []\n",
    "point_mask = []\n",
    "point_user_mask = []\n",
    "for i,row in df.iterrows():\n",
    "    pt = row['coordinates']['type']\n",
    "    place_type_all[pt]+=1\n",
    "    if pt ==\"Point_Place\":\n",
    "        point_place_mask.append(i)\n",
    "    elif pt == \"Point\":\n",
    "        point_mask.append(i)\n",
    "    elif pt==\"Point_User\":\n",
    "        point_user_mask.append(i)\n",
    "    else:\n",
    "        print('error')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_type_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = Counter()\n",
    "for i,row in df.iterrows():\n",
    "    user_id = row['user']['id']\n",
    "    user_name = row['user']['name']\n",
    "    user_counts[user_id]+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = pd.to_datetime('01 Feb 2020', format = '%d %b %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['created_at']>=yesterday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_counts = Counter()\n",
    "for i,row in df.iterrows():\n",
    "    location = tuple(row['coordinates']['coordinates'])\n",
    "    location_counts[location] +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location_type'] = df.apply(lambda x: x['coordinates']['type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = df[df['location_type']== \"Point_User\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[((-33.8688, 151.2093), 29042),\n",
    " ((-37.810151, 144.965594), 26220),\n",
    " ((-27.4698, 153.0251), 20256),\n",
    " ((-31.9505, 115.8605), 16001),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city(city_coordinates):\n",
    "    \n",
    "    if city_coordinates == [-33.8688, 151.2093]:\n",
    "        return 'Sydney'\n",
    "    elif city_coordinates == [-37.810151, 144.965594]:\n",
    "        return 'Melbourne'\n",
    "    \n",
    "    elif city_coordinates == [-27.4698, 153.0251]:\n",
    "        return 'Brisbane'\n",
    "\n",
    "    elif city_coordinates == [-31.9505, 115.8605]:\n",
    "        return 'Perth'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities['City'] = df_cities.apply(lambda x: get_city(x['coordinates']['coordinates']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "\n",
    "def get_ngrams(df, n):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stop_words.add(\"https\")\n",
    "\n",
    "    total_counts = Counter()\n",
    "    tweet_counts = Counter()\n",
    "    \n",
    "    raw_tweet_texts = df[\"full_text\"].values\n",
    "    \n",
    "    for i, tweet in enumerate(raw_tweet_texts):\n",
    "        \n",
    "        word_tokens = word_tokenize(tweet.lower()) \n",
    "        \n",
    "        filtered_sentence = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words and w not in string.punctuation] \n",
    "        filtered_sentence_set = set(filtered_sentence)\n",
    "    \n",
    "        total_counts.update(filtered_sentence)\n",
    "        tweet_counts.update(filtered_sentence_set)\n",
    "        \n",
    "    return tweet_counts\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melb = df_cities[df_cities[\"City\"]=='Melbourne'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts_melb = get_ngrams(df_melb,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts_melb.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Melbourne\",\"Sydney\", \"Brisbane\", \"Perth\"]\n",
    "cities_tweet_counts = dict()\n",
    "df_ = dict()\n",
    "for city in cities:\n",
    "    df_[city] = df_cities[df_cities[\"City\"]==city].copy()\n",
    "    cities_tweet_counts[city] = get_ngrams(df_[city],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[\"Melbourne\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_tweet_counts[\"Melbourne\"].most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[\"Sydney\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_tweet_counts[\"Sydney\"].most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_tweet_counts[\"Brisbane\"].most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_tweet_counts[\"Perth\"].most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_topics(n):\n",
    "    \n",
    "    common_topics = set([x for x,y in cities_tweet_counts[\"Melbourne\"].most_common(n)])\n",
    "    for city in cities:\n",
    "        common_topics = common_topics.intersection(set([x for x,y in cities_tweet_counts[city].most_common(n)]))\n",
    "        \n",
    "    return common_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_common_topics(200) ['app','auspol','business','care','case','china','chinese','community','confirmed', 'covidsafe','death','distancing','economy','economic','hospital','infection','job','lockdown','minister','morrison','outbreak','pandemic','positive','response','restriction','safe','school','scottmorissonmp','vaccine','virus','work','worker','working']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keywords = ['app','auspol','business','care','case','china','chinese','community','confirmed'\n",
    "                        , 'covidsafe','death','distancing','economy','economic','hospital','infection','job','lockdown',\n",
    "                        'minister','morrison','outbreak','pandemic','positive','response','restriction','safe','school',\n",
    "                        'scottmorissonmp','vaccine','virus','work','worker','working']\n",
    "df_rows= []\n",
    "for keyword in keywords:\n",
    "    for city in cities:\n",
    "        count = cities_tweet_counts[city][keyword]/df_[city].shape[0]*100\n",
    "        row = [keyword, count, city]\n",
    "        df_rows.append(row)\n",
    "        \n",
    "        \n",
    "keyword_df = pd.DataFrame(df_rows, columns = [\"keyword\", \"count\", \"city\"] )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x=\"keyword\", y=\"count\", hue=\"city\", data=keyword_df, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "#string.punctuation\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.add(\"https\")\n",
    "\n",
    "total_counts = Counter()\n",
    "tweet_counts = Counter()\n",
    "\n",
    "raw_tweet_texts = df[\"full_text\"].values\n",
    "cv_tokens =  Counter()\n",
    "for i, tweet in enumerate(raw_tweet_texts):\n",
    "    \n",
    "    word_tokens = word_tokenize(tweet.lower()) \n",
    "    if 'coronavirus' in word_tokens:\n",
    "       # print(word_tokens)\n",
    "        #print(tweet)\n",
    "        word_tokens2 = tweet.lower().split(\" \")\n",
    "        for token in word_tokens2:\n",
    "            if 'coronavirus' in token:\n",
    "                cv_tokens[token] +=1\n",
    "        #print(\"\\n\\n\")\n",
    "       \n",
    "    filtered_sentence = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words and w not in string.punctuation] \n",
    "    filtered_sentence_set = set(filtered_sentence)\n",
    "    \n",
    "    total_counts.update(filtered_sentence)\n",
    "    tweet_counts.update(filtered_sentence_set)\n",
    "    \n",
    "    if i % 50000 == 0:\n",
    "        print('tweet: ', i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "#string.punctuation\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.add(\"https\")\n",
    "\n",
    "bi_total_counts = Counter()\n",
    "bi_tweet_counts = Counter()\n",
    "\n",
    "raw_tweet_texts = df[\"full_text\"].values\n",
    "\n",
    "for i, tweet in enumerate(raw_tweet_texts):\n",
    "    \n",
    "    word_tokens = word_tokenize(tweet.lower()) \n",
    "    #word_tokens = tweet.split(\" \")\n",
    "    #print(word_tokens)\n",
    "    sent = \" \".join([lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words and w not in string.punctuation])\n",
    "    #print(sent)\n",
    "    bigram_tokens = generate_ngrams(sent,2)\n",
    "    bigram_tokens_set = set(bigram_tokens)\n",
    "    \n",
    "    bi_total_counts.update(bigram_tokens)\n",
    "    bi_tweet_counts.update(bigram_tokens_set)\n",
    "    \n",
    "    if i % 50000 == 0:\n",
    "        print('tweet: ', i)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
